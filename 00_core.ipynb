{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuModel\n",
    "\n",
    "> Establish a muzero model using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports \n",
    "\n",
    "from nbdev.showdoc import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from typing import Tuple, List, Union\n",
    "\n",
    "def set_seed(seed: int=47) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# Stacks the columns on top of each other. Union lets us define two types cool!\n",
    "def bstack(bb: List[Union[float, np.ndarray]]) -> List[np.ndarray]:\n",
    "    return [np.array([i[j] for i in bb]).reshape(len(bb), -1) for j in range(len(bb[0]))]\n",
    "\n",
    "def to_one_hot(a: np.ndarray, K: int, a_dim: int) -> np.ndarray:\n",
    "    # Vectorized version of one hot encoding.\n",
    "    # K is number of steps and a_dim is the action dimension.\n",
    "    one_hot_action = np.zeros((K * a_dim))\n",
    "    index = np.arange(0, K * a_dim, a_dim)\n",
    "    # Removes negative actions\n",
    "    index = index[a >= 0]\n",
    "    one_hot_action[a[a >= 0] + index] = 1\n",
    "    return np.split(one_hot_action, K)\n",
    "\n",
    "def reformat_batch(batch: np.ndarray, K: int, a_dim: int, remove_policy=False) -> Tuple[List[np.ndarray]]:\n",
    "    X, Y = [], []\n",
    "    for o, a, outs in batch:\n",
    "        a = np.array(a)\n",
    "        x = [o] + to_one_hot(a, K, a_dim)\n",
    "        \n",
    "        # Flatten outs into a list for targets.\n",
    "        y = [item for sublist in outs for item in sublist]\n",
    "        \n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        \n",
    "        X, Y = bstack(X), bstack(Y)\n",
    "        \n",
    "        if remove_policy:\n",
    "            nY = [Y[0]]\n",
    "            # The model predicts three future quantities.\n",
    "            for i in range(3, len(Y), 3):\n",
    "                nY.append(Y[i])\n",
    "                nY.append(Y[i+1])\n",
    "            Y = nY\n",
    "        else:\n",
    "            # Removes the reward prediction??\n",
    "            Y.pop(1)\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "\n",
    "class DenseRepresentation(nn.Module):\n",
    "    # h network: Gets the representation for the initial hidden state s^0 from past observations.\n",
    "    def __init__(self, o_dim: int, s_dim: int, hidden_layer: int, hidden_layer_count: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input linear layer with elu activation function.\n",
    "        sequential = [nn.Linear(o_dim, hidden_layer_dim), nn.ELU()]\n",
    "        # Hidden llinear layers with elu activation function.\n",
    "        sequential += [nn.Linear(hidden_layer_dim, hidden_layer_dim), nn.ELU()] * hidden_layer_count\n",
    "        # Output linear layer according to state dimension.\n",
    "        self.out_state = nn.Linear(hidden_layer_dim, s_dim)\n",
    "        \n",
    "        # Star notation makes the function take in a tuple :)\n",
    "        self.linearReluStack = nn.Sequential(*tuple(sequential))\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linearReluStack(state)\n",
    "        return self.out_state(x)\n",
    "\n",
    "class DenseDynamic(nn.Module):\n",
    "    # g network: Produces an immediate reward r and and a new hidden state s.\n",
    "    def __init__(self, s_dim: int, a_dim: int, hidden_layer_dim: int, hidden_layer_count: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input and hidden linear layers with elu activation function.\n",
    "        sequential = [nn.Linear(s_dim + a_dim, hidden_layer_dim), nn.ELU()]\n",
    "        sequential += [nn.Linear(hidden_layer_dim, hidden_layer_dim), nn.ELU()] * hidden_layer_count\n",
    "        \n",
    "        self.linearReluStack = nn.Sequential(*tuple(sequential))\n",
    "        self.out_reward = nn.Linear(hidden_layer_dim, 1)\n",
    "        self.out_state = nn.Linear(hidden_layer_dim, s_dim)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Why are the dimensions reversed?? Takes the latest state and action first??? Then reverse it again. \n",
    "        x = self.linearRelueStack(state.T, action.T).T\n",
    "        return self.out_reward(x), self.out_state(x)\n",
    "    \n",
    "class DensePrediction(nn.Module):\n",
    "    # f network: Computes the policy p^k and and value function v^k from the hidden state s^k.\n",
    "    def __init__(self, s_dim: int, a_dim: int, hidden_layer_dim: int, hidden_layer_count: int, \n",
    "                 with_policy: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.with_policy = with_policy\n",
    "        \n",
    "        sequential = [nn.Linear(s_dim, hidden_layer_dim), nn.ELU()]\n",
    "        sequential += [nn.Linear(hidden_layer_dim, hidden_layer_dim), nn.ELU()] * hidden_layer_count\n",
    "        \n",
    "        self.linearReluStack = nn.Sequential(*tuple(sequential))\n",
    "        self.out_policy = nn.Linear(hidden_layer_dim, a_dim)\n",
    "        self.out_value = nn.Linear(hidden_layer_dim, 1)\n",
    "        \n",
    "        def forward(self, state: torch.Tensor) -> Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "            x = self.linearReluStack(state)\n",
    "            if self.with_policy:\n",
    "                return self.out_policy(x), self.out_value(x)\n",
    "            else:\n",
    "                return self.out_value(x)\n",
    "\n",
    "class MuModel:\n",
    "    # these are static variables for this class\n",
    "    LAYER_COUNT = 4\n",
    "    LAYER_DIM = 128\n",
    "    BN = False\n",
    "    \n",
    "    def __init__(self, observation_dim: int, action_dim: int, s_dim: int=8, K: int=5, lr: float=1e-3, \n",
    "                 with_policy: bool=True, device='cpu') -> None:\n",
    "        \n",
    "        self.observation_dim, self.action_dim, self.s_dim = observation_dim, action_dim, s_dim\n",
    "        self.K, self.lr, self.with_policy = K, lr, with_policy\n",
    "        self.device = device\n",
    "        \n",
    "        # h: Representation function.\n",
    "        self.h = DenseRepresentation(o_dim=observation_dim[0], s_dim=s_dim, hidden_layer_dim=self.LAYER_DIM, \n",
    "                                     hidden_layer_count=self.LAYER_COUNT).to(device)\n",
    "        # g: Dynamic function.\n",
    "        self.g = DenseDynamic(s_dim=s_dim, a_dim=action_dim, hidden_layer_dim=self.LAYER_DIM,\n",
    "                              hidden_layer_count=self.LAYER_COUNT).to(device)\n",
    "        # f: Predictive function.\n",
    "        self.f = DensePrediction(s_dim=s_dim, a_dim=action_dim, hidden_layer_dim=self.LAYER_DIM,\n",
    "                                hidden_layer_count=self.LAYER_COUNT, with_policy=with_policy).to(device)\n",
    "        \n",
    "        params = list(self.h.parameters()) + list(self.g.parameters()) + list(self.f.parameters())\n",
    "        self.optimizer = optim.Adam(params, lr=self.lr)\n",
    "        self.losses = [] # Overall loss\n",
    "        \n",
    "        # Make class compatible with Geohot's other code.\n",
    "        self.o_dim = self.observation_dim\n",
    "        self.a_dim = self.action_dim\n",
    "        Mu = namedtuple('mu', 'predict')\n",
    "        self.mu = Mu(self.predict)\n",
    "        \n",
    "    def forward(self, X: List[torch.Tensor], train: bool=True) -> List[torch.Tensor]:\n",
    "        # Sets the models into evaluation mode.\n",
    "        self.h.eval(), self.g.eval(), self.f.eval()\n",
    "        # Sets the models into training mode.\n",
    "        if train:\n",
    "            self.h.train(), self.g.train(), self.f.train()\n",
    "        \n",
    "        # Converts features from numpy array to tensor type. \n",
    "        X = [torch.from_numpy(x.astype(np.float32)) for x in X]\n",
    "        Y_pred = []\n",
    "        \n",
    "        # Gets the initial hidden state. \n",
    "        state = self.h(X[0])\n",
    "        if self.with_policy:\n",
    "            policy, value = self.f(state)\n",
    "            Y_pred += [value, policy]\n",
    "        else:\n",
    "            value = self.f(state)\n",
    "            Y_pred.append(value)\n",
    "        \n",
    "        # Make predictions for K time-steps.\n",
    "        for k in range(self.K):\n",
    "            reward, new_state = self.g(state, X[k+1])\n",
    "            if self.with_policy:\n",
    "                policy, value = self.f(state)\n",
    "                Y_pred += [value, reward, policy]\n",
    "            else:\n",
    "                value = self.f(state)\n",
    "                Y_pred += [value, reward]\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        return Y_pred\n",
    "        \n",
    "            \n",
    "    def predict(self, X: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        with torch.no_grad():\n",
    "            Y_pred = self.forward(X, train=False)\n",
    "        return Y_pred\n",
    "    \n",
    "    def train(self, batch: List[np.ndarray]) -> None:\n",
    "        # Sets the models into training mode.\n",
    "        self.h.train(), self.g.train(), self.f.train()\n",
    "        batch_losses = []\n",
    "        # Could probably swap F.mse_loss to nn.MSELoss()\n",
    "        mse, smcel = F.mse_loss, nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        X, Y = reformat_batch(batch, self.K, self.action_dim, remove_policy=not self.with_policy)\n",
    "        Y = [torch.from_numpy(y.astype(np.float32)).to(self.device) for y in Y]\n",
    "        Y_pred = self.forward(X, train=True)\n",
    "        \n",
    "        # Calculates value/policy error from initial hidden state.\n",
    "        batch_losses.append(mse(Y_pred[0], Y[0]))\n",
    "        if self.with_policy:\n",
    "            losses.append(smcel(Y_pred[2], Y[2]))\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            batch_losses.append(mse(Y_pred[3*k + 2]), Y[3*k + 2]) # Value \n",
    "            batch_losses.append(mse(Y_pred[3*k + 3]), Y[3*k + 3]) # Reward\n",
    "            if self.with_policy:\n",
    "                losses.append(mse(Y_pred[3*k + 4]), Y[3*k + 4]) # Policy\n",
    "            \n",
    "        loss = sum(batch_losses)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append([loss.item()] + [a.item() for a in batch_losses])\n",
    "    \n",
    "    def ht(self, state: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            if not torch.is_tensor(state):\n",
    "                state = torch.from_numpy(state.astype(np.float32)).to(self.device)\n",
    "            return self.h(state)\n",
    "               \n",
    "    def ft(self, state: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            if not torch.is_tensor(state):\n",
    "                state = torch.from_numpy(state.astype(np.float32)).to(self.device)\n",
    "            \n",
    "            if self.with_policy:\n",
    "                policy, value = self.f(state)\n",
    "                return policy.exp(), value\n",
    "            else:\n",
    "                value = self.f(state)\n",
    "                return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 4, 1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = np.array([[1, 2, 3], [4, 4, 1]])\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1],\n",
       "        [4]]),\n",
       " array([[2],\n",
       "        [4]]),\n",
       " array([[3],\n",
       "        [1]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bstack(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
